# ===== IMPORT REQUIRED LIBRARIES =====
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ===== 3.1 BUILDING AND TESTING HELPER FUNCTIONS =====

# 1. Implementing Sigmoid/Logistic Function
def logistic_function(x):
    """
    Computes the logistic function applied to any value of x.
    Arguments:
        x: scalar or numpy array of any size.
    Returns:
        y: logistic function applied to x.
    """
    y = 1 / (1 + np.exp(-x))
    return y

# 2. Implementing Log Loss Function
def log_loss(y_true, y_pred):
    """
    Computes log loss for true target value y = {0 or 1} and predicted target value y' inbetween {0-1}.
    Arguments:
        y_true (scalar): true target value {0 or 1}.
        y_pred (scalar): predicted target value {0-1}.
    Returns:
        loss (float): loss/error value
    """
    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)
    loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)
    return loss

# 3. Implementing Cost Function
def cost_function(y_true, y_pred):
    """
    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)
    Args:
        y_true (array_like, shape (n,)): array of true values (0 or 1)
        y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)
    Returns:
        cost (float): nonnegative cost corresponding to y_true and y_pred
    """
    assert len(y_true) == len(y_pred), "Length of true values and length of predicted values do not match"
    n = len(y_true)
    loss_vec = log_loss(y_true, y_pred)
    cost = np.sum(loss_vec) / n
    return cost

# 4. Extending the cost function for sigmoid regression to be used with model parameters
def costfunction_logreg(X, y, w, b):
    """
    Computes the cost function, given data and model parameters.
    Args:
        X (ndarray, shape (m,n)): data on features, m observations with n features.
        y (array_like, shape (m,)): array of true values of target (0 or 1).
        w (array_like, shape (n,)): weight parameters of the model.
        b (float): bias parameter of the model.
    Returns:
        cost (float): nonnegative cost corresponding to y and y_pred.
    """
    n, d = X.shape
    assert len(y) == n, "Number of feature observations and number of target observations do not match."
    assert len(w) == d, "Number of features and number of weight parameters do not match."
    
    # Compute z using np.dot
    z = np.dot(X, w) + b
    
    # Compute predictions using logistic function (sigmoid)
    y_pred = logistic_function(z)
    
    # Compute the cost using the cost function
    cost = cost_function(y, y_pred)
    return cost

# 5.1 Implementing Gradient Computation
def compute_gradient(X, y, w, b):
    """
    Computes gradients of the cost function with respect to model parameters.
    Args:
        X (ndarray, shape (n,d)): Input data, n observations with d features
        y (array_like, shape (n,)): True labels (0 or 1)
        w (array_like, shape (d,)): Weight parameters of the model
        b (float): Bias parameter of the model
    Returns:
        grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight parameters
        grad_b (float): Gradient of the cost function with respect to the bias parameter
    """
    n, d = X.shape
    assert len(y) == n, f"Expected y to have {n} elements, but got {len(y)}"
    assert len(w) == d, f"Expected w to have {d} elements, but got {len(w)}"
    
    # Compute predictions using logistic function (sigmoid)
    z = np.dot(X, w) + b
    y_pred = logistic_function(z)
    
    # Compute gradients
    error = y_pred - y
    grad_w = np.dot(X.T, error) / n
    grad_b = np.sum(error) / n
    
    return grad_w, grad_b

# 5.2 Implementing Gradient Descent
def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):
    """
    Implements batch gradient descent to optimize logistic regression parameters.
    Args:
        X (ndarray, shape (n,d)): Data on features, n observations with d features
        y (array_like, shape (n,)): True values of target (0 or 1)
        w (array_like, shape (d,)): Initial weight parameters
        b (float): Initial bias parameter
        alpha (float): Learning rate
        n_iter (int): Number of iterations
        show_cost (bool): If True, displays cost every 100 iterations
        show_params (bool): If True, displays parameters every 100 iterations
    Returns:
        w (array_like, shape (d,)): Optimized weight parameters
        b (float): Optimized bias parameter
        cost_history (list): List of cost values over iterations
        params_history (list): List of parameters (w, b) over iterations
    """
    n, d = X.shape
    assert len(y) == n, "Number of observations in X and y do not match"
    assert len(w) == d, "Number of features in X and w do not match"
    
    cost_history = []
    params_history = []
    
    for i in range(n_iter):
        # Compute gradients
        grad_w, grad_b = compute_gradient(X, y, w, b)
        
        # Update weights and bias
        w = w - alpha * grad_w
        b = b - alpha * grad_b
        
        # Compute cost
        cost = costfunction_logreg(X, y, w, b)
        
        # Store cost and parameters
        cost_history.append(cost)
        params_history.append((w.copy(), b))
        
        # Optionally print cost and parameters
        if show_cost and (i % 100 == 0 or i == n_iter - 1):
            print(f"Iteration {i}: Cost = {cost:.6f}")
        if show_params and (i % 100 == 0 or i == n_iter - 1):
            print(f"Iteration {i}: w = {w}, b = {b:.6f}")
    
    return w, b, cost_history, params_history

# 6. Decision/Prediction Function
def prediction(X, w, b, threshold=0.5):
    """
    Predicts binary outcomes for given input features based on logistic regression parameters.
    Arguments:
        X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d features.
        w (ndarray, shape (d,)): Array of weights learned via gradient descent.
        b (float): Bias learned via gradient descent.
        threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.
    Returns:
        y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).
    """
    # Compute the predicted probabilities using the logistic function
    z = np.dot(X, w) + b
    y_test_prob = logistic_function(z)
    
    # Classify based on the threshold
    y_pred = (y_test_prob >= threshold).astype(int)
    
    return y_pred

# 7. Evaluation Function
def evaluate_classification(y_true, y_pred):
    """
    Computes the confusion matrix, precision, recall, and F1-score for binary classification.
    Arguments:
        y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).
        y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).
    Returns:
        metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.
    """
    # Initialize confusion matrix components
    TP = np.sum((y_true == 1) & (y_pred == 1))  # True Positives
    TN = np.sum((y_true == 0) & (y_pred == 0))  # True Negatives
    FP = np.sum((y_true == 0) & (y_pred == 1))  # False Positives
    FN = np.sum((y_true == 1) & (y_pred == 0))  # False Negatives
    
    # Confusion matrix
    confusion_matrix = np.array([[TN, FP],
                                 [FN, TP]])
    
    # Precision, recall, and F1-score
    precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0
    recall = TP / (TP + FN) if (TP + FN) > 0.0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0.0 else 0.0
    
    # Metrics dictionary
    metrics = {
        "confusion_matrix": confusion_matrix,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score
    }
    
    return metrics

# ===== RUNNING TEST CASES =====
print("=== Testing Helper Functions ===\n")

# Test 1: Logistic Function
print("1. Testing logistic_function...")
import numpy as np

def test_logistic_function():
    # Test with scalar input
    x_scalar = 0
    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)  # Expected output: 0.5
    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, "Test failed for scalar input"
    
    # Test with positive scalar input
    x_pos = 2
    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)  # Expected output: ~0.881
    assert round(logistic_function(x_pos), 3) == expected_output_pos, "Test failed for positive scalar input"
    
    # Test with negative scalar input
    x_neg = -3
    expected_output_neg = round(1 / (1 + np.exp(3)), 3)  # Expected output: ~0.047
    assert round(logistic_function(x_neg), 3) == expected_output_neg, "Test failed for negative scalar input"
    
    # Test with numpy array input
    x_array = np.array([0, 2, -3])
    expected_output_array = np.array([0.5, 0.881, 0.047])  # Adjusted expected values rounded to 3 decimals
    # Use np.round to round the array element-wise and compare
    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), "Test failed for numpy array input"
    
    print("All tests passed!")

test_logistic_function()

# Test 2: Log Loss Function
print("\n2. Testing log_loss...")

def test_log_loss():
    import numpy as np
    
    # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)
    y_true = 1
    y_pred = 1
    expected_loss = 0.0  # Log loss is 0 for perfect prediction
    assert np.isclose(log_loss(y_true, y_pred), expected_loss), "Test failed for perfect prediction (y_true=1, y_pred=1)"
    
    # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)
    y_true = 0
    y_pred = 0
    expected_loss = 0.0  # Log loss is 0 for perfect prediction
    assert np.isclose(log_loss(y_true, y_pred), expected_loss), "Test failed for perfect prediction (y_true=0, y_pred=0)"
    
    # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)
    # Note: After clipping, log(0) is avoided, so this won't raise an error
    y_true = 1
    y_pred = 0
    loss = log_loss(y_true, y_pred)
    assert not np.isnan(loss), "Test failed for y_true=1, y_pred=0"
    
    # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)
    y_true = 0
    y_pred = 1
    loss = log_loss(y_true, y_pred)
    assert not np.isnan(loss), "Test failed for y_true=0, y_pred=1"
    
    # Test case 5: Partially correct prediction
    y_true = 1
    y_pred = 0.8
    expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2))  # ~0.2231
    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), "Test failed for partially correct prediction (y_true=1, y_pred=0.8)"
    
    y_true = 0
    y_pred = 0.2
    expected_loss = -(0 * np.log(0.2)) - (1 * np.log(0.8))  # ~0.2231
    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), "Test failed for partially correct prediction (y_true=0, y_pred=0.2)"
    
    print("All tests passed!")

test_log_loss()

# Test 3: Cost Function
print("\n3. Testing cost_function...")

def test_cost_function():
    # Test case 1: Simple example with known expected cost
    y_true = np.array([1, 0, 1])
    y_pred = np.array([0.9, 0.1, 0.8])
    
    # Expected output: Manually calculate cost for these values
    expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +
                     -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +
                     -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3
    
    # Call the cost_function to get the result
    result = cost_function(y_true, y_pred)
    
    # Assert that the result is close to the expected cost with a tolerance of 1e-6
    assert np.isclose(result, expected_cost, atol=1e-6), f"Test failed: {result} != {expected_cost}"
    print("Test passed for simple case!")

test_cost_function()

# Test 4: Cost Function for Logistic Regression
print("\n4. Testing costfunction_logreg...")

# Testing the Function:
X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1
print(f"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}")

# Test 5: Gradient Computation
print("\n5. Testing compute_gradient...")

# Simple test case
X = np.array([[10, 20], [-10, 10]])  # shape (2, 2)
y = np.array([1, 0])  # shape (2,)
w = np.array([0.5, 1.5])  # shape (2,)
b = 1  # scalar

# Assertion tests
try:
    grad_w, grad_b = compute_gradient(X, y, w, b)
    print("Gradients computed successfully.")
    print(f"grad_w: {grad_w}")
    print(f"grad_b: {grad_b}")
except AssertionError as e:
    print(f"Assertion error: {e}")

# Test 6: Gradient Descent
print("\n6. Testing gradient_descent...")

def test_gradient_descent():
    X = np.array([[0.1, 0.2], [-0.1, 0.1]])  # Shape (2, 2)
    y = np.array([1, 0])  # Shape (2,)
    w = np.zeros(X.shape[1])  # Shape (2,)
    b = 0.0  # Scalar
    alpha = 0.1  # Learning rate
    n_iter = 100  # Number of iterations
    
    # Run gradient descent
    w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=False)
    
    # Assertions
    assert len(cost_history) == n_iter, "Cost history length does not match the number of iterations"
    assert w_out.shape == w.shape, "Shape of output weights does not match the initial weights"
    assert isinstance(b_out, float), "Bias output is not a float"
    assert cost_history[-1] < cost_history[0], "Cost did not decrease over iterations"
    print("All tests passed!")

test_gradient_descent()

# Test 7: Prediction Function
print("\n7. Testing prediction...")

def test_prediction():
    X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]])  # Shape (3, 2)
    w_test = np.array([1.0, -1.0])  # Shape (2,)
    b_test = 0.0  # Scalar bias
    threshold = 0.5  # Default threshold
    
    # Calculate expected output manually:
    # For [0.5, 1.0]: z = 0.5*1 + 1.0*(-1) = -0.5, sigmoid = 0.3775 -> 0
    # For [1.5, -0.5]: z = 1.5*1 + (-0.5)*(-1) = 2.0, sigmoid = 0.8808 -> 1
    # For [-0.5, -1.0]: z = -0.5*1 + (-1.0)*(-1) = 0.5, sigmoid = 0.6225 -> 1
    expected_output = np.array([0, 1, 1])
    
    # Call the prediction function
    y_pred = prediction(X_test, w_test, b_test, threshold)
    
    # Assert that the output matches the expected output
    assert np.array_equal(y_pred, expected_output), f"Expected {expected_output}, but got {y_pred}"
    print("Test passed!")

test_prediction()

# Test 8: Evaluation Function
print("\n8. Testing evaluate_classification...")

def test_evaluate_classification():
    y_true = np.array([1, 0, 1, 0, 1, 1, 0, 0])
    y_pred = np.array([1, 0, 0, 0, 1, 1, 1, 0])
    
    metrics = evaluate_classification(y_true, y_pred)
    
    print("Confusion Matrix:")
    print(metrics["confusion_matrix"])
    print(f"Precision: {metrics['precision']:.3f}")
    print(f"Recall: {metrics['recall']:.3f}")
    print(f"F1-Score: {metrics['f1_score']:.3f}")

test_evaluate_classification()

# ===== 3.2 PUTTING HELPER FUNCTIONS TO ACTION =====
print("\n" + "="*60)
print("3.2 Implementing Sigmoid Regression on Pima Indians Diabetes Dataset")
print("="*60 + "\n")

# 1. Loading and Preparing the Data
print("1. Loading and preparing data...")

# Load dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 
           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']

data_pima_diabetes = pd.read_csv(url, names=columns)

# Data cleaning
columns_to_clean = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
data_pima_diabetes[columns_to_clean] = data_pima_diabetes[columns_to_clean].replace(0, np.nan)
data_pima_diabetes.fillna(data_pima_diabetes.median(), inplace=True)

print(f"Dataset shape: {data_pima_diabetes.shape}")
print("\nData Info:")
data_pima_diabetes.info()
print("\nSummary Statistics:")
print(data_pima_diabetes.describe())

# 2. Train-Test Split and Scaling
print("\n2. Splitting and scaling data...")

X = data_pima_diabetes.drop(columns=['Outcome']).values
y = data_pima_diabetes['Outcome'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Training set size: {X_train_scaled.shape}")
print(f"Test set size: {X_test_scaled.shape}")

# 3. Training the Sigmoid Regression Model
print("\n3. Training the Logistic Regression Model...")

# Initialize parameters
w = np.zeros(X_train_scaled.shape[1])
b = 0.0
alpha = 0.1
n_iter = 1000

# Train model
print("\nTraining Logistic Regression Model:")
w, b, cost_history, params_history = gradient_descent(
    X_train_scaled, y_train, w, b, alpha, n_iter, 
    show_cost=True, show_params=False
)

# Plot cost history
plt.figure(figsize=(9, 6))
plt.plot(cost_history)
plt.xlabel("Iteration", fontsize=14)
plt.ylabel("Cost", fontsize=14)
plt.title("Cost vs Iteration During Training", fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 4. Evaluating Model Performance
print("\n4. Evaluating model performance...")

# Test model
y_train_pred = prediction(X_train_scaled, w, b)
y_test_pred = prediction(X_test_scaled, w, b)

# Evaluate train and test performance
train_cost = costfunction_logreg(X_train_scaled, y_train, w, b)
test_cost = costfunction_logreg(X_test_scaled, y_test, w, b)
print(f"\nTrain Loss (Cost): {train_cost:.4f}")
print(f"Test Loss (Cost): {test_cost:.4f}")

# Check for overfitting/underfitting
if train_cost < test_cost and (test_cost - train_cost) > 0.1:
    print("Model may be overfitting (train cost significantly lower than test cost)")
elif train_cost > test_cost:
    print("Model may be underfitting (train cost higher than test cost)")
else:
    print("Model shows reasonable generalization (train and test costs are close)")

# 5. Classification Metrics
print("\n5. Classification metrics...")

# Accuracy on test data
test_accuracy = np.mean(y_test_pred == y_test) * 100
train_accuracy = np.mean(y_train_pred == y_train) * 100
print(f"Train Accuracy: {train_accuracy:.2f}%")
print(f"Test Accuracy: {test_accuracy:.2f}%")

# Evaluation using our evaluation function
metrics = evaluate_classification(y_test, y_test_pred)
print(f"\nConfusion Matrix:\n{metrics['confusion_matrix']}")
print(f"Precision: {metrics['precision']:.3f}")
print(f"Recall: {metrics['recall']:.3f}")
print(f"F1-Score: {metrics['f1_score']:.3f}")

# Visualizing Confusion Matrix
conf_matrix = metrics['confusion_matrix']
fig, ax = plt.subplots(figsize=(6, 6))
ax.imshow(conf_matrix, cmap='Blues')
ax.grid(False)
ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))
ax.set_ylim(1.5, -0.5)

for i in range(2):
    for j in range(2):
        ax.text(j, i, conf_matrix[i, j], ha='center', va='center', 
                color='white' if conf_matrix[i, j] > conf_matrix.max()/2 else 'black',
                fontsize=14, fontweight='bold')

plt.title("Confusion Matrix", fontsize=14)
plt.tight_layout()
plt.show()

# 6. Additional Insights
print("\n6. Additional insights...")

# Feature importance based on weights
feature_names = columns[:-1]  # Exclude 'Outcome'
feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Weight': w,
    'Absolute_Weight': np.abs(w)
}).sort_values('Absolute_Weight', ascending=False)

print("\nFeature Importance (based on absolute weight values):")
print(feature_importance)

# Probability distribution
z_test = np.dot(X_test_scaled, w) + b
y_test_prob = logistic_function(z_test)

plt.figure(figsize=(10, 6))
plt.hist(y_test_prob[y_test == 0], bins=20, alpha=0.5, label='Class 0 (No Diabetes)', color='blue')
plt.hist(y_test_prob[y_test == 1], bins=20, alpha=0.5, label='Class 1 (Diabetes)', color='red')
plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold (0.5)')
plt.xlabel('Predicted Probability', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.title('Distribution of Predicted Probabilities', fontsize=14)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("SUMMARY")
print("="*60)
print(f"1. Model trained successfully with {n_iter} iterations")
print(f"2. Final training cost: {train_cost:.4f}")
print(f"3. Final test cost: {test_cost:.4f}")
print(f"4. Test accuracy: {test_accuracy:.2f}%")
print(f"5. Precision: {metrics['precision']:.3f}")
print(f"6. Recall: {metrics['recall']:.3f}")
print(f"7. F1-Score: {metrics['f1_score']:.3f}")
print("="*60)

print("\nAll tasks completed successfully!")