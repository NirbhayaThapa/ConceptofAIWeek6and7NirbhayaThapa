# ======================
# 1. IMPORTS
# ======================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score

# ======================
# 2. HELPER FUNCTIONS
# ======================

# 2.1 Softmax Function
def softmax(z):
    """
    Compute softmax probabilities for each row of z.
    """
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# 2.2 Categorical Cross-Entropy Loss (Single Observation)
def loss_softmax(y_true, y_pred):
    """
    Compute cross-entropy loss for a single observation.
    """
    return -np.sum(y_true * np.log(y_pred + 1e-10))

# 2.3 Cost Function (Average Loss)
def cost_softmax(X, y, W, b):
    """
    Compute average cross-entropy cost over all samples.
    """
    n = X.shape[0]
    z = np.dot(X, W) + b
    y_pred = softmax(z)
    return -np.sum(y * np.log(y_pred + 1e-10)) / n

# 2.4 Gradient Computation
def compute_gradient_softmax(X, y, W, b):
    """
    Compute gradients w.r.t. weights and biases.
    """
    n = X.shape[0]
    z = np.dot(X, W) + b
    y_pred = softmax(z)
    grad_W = np.dot(X.T, (y_pred - y)) / n
    grad_b = np.sum(y_pred - y, axis=0) / n
    return grad_W, grad_b

# 2.5 Gradient Descent
def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):
    """
    Perform gradient descent optimization.
    """
    cost_history = []
    for i in range(n_iter):
        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)
        W -= alpha * grad_W
        b -= alpha * grad_b
        cost = cost_softmax(X, y, W, b)
        cost_history.append(cost)
        if show_cost and (i % 100 == 0 or i == n_iter - 1):
            print(f"Iteration {i}: Cost = {cost:.6f}")
    return W, b, cost_history

# 2.6 Prediction Function
def predict_softmax(X, W, b):
    """
    Predict class labels.
    """
    z = np.dot(X, W) + b
    y_pred = softmax(z)
    return np.argmax(y_pred, axis=1)

# 2.7 Evaluation Function
def evaluate_classification(y_true, y_pred):
    """
    Compute confusion matrix, precision, recall, F1-score.
    """
    cm = confusion_matrix(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average="weighted", zero_division=0)
    recall = recall_score(y_true, y_pred, average="weighted", zero_division=0)
    f1 = f1_score(y_true, y_pred, average="weighted", zero_division=0)
    return cm, precision, recall, f1

# ======================
# 3. TESTING HELPER FUNCTIONS
# ======================

# 3.1 Test Softmax
def test_softmax():
    test_cases = [
        (np.array([[0, 0, 0]]), "All zeros"),
        (np.array([[1, 2, 3]]), "Simple case"),
        (np.array([[1000, 1000, 1000]]), "Large identical values"),
        (np.array([[-1000, -1000, -1000]]), "Small identical values"),
        (np.array([[1, 0, -1]]), "Mixed positive and negative")
    ]
    for z, desc in test_cases:
        result = softmax(z)
        assert np.allclose(result.sum(axis=1), 1), f"Failed sum-to-1: {desc}"
        assert np.all(result >= 0), f"Failed non-negativity: {desc}"
    print("All softmax tests passed.")

# 3.2 Test Loss Function
def test_loss_softmax():
    y_true = np.array([0, 1, 0])
    y_pred = np.array([0.1, 0.8, 0.1])
    expected = -np.log(0.8)
    assert np.isclose(loss_softmax(y_true, y_pred), expected), "Loss test failed"
    print("Loss function test passed.")

# 3.3 Test Cost Function
def test_cost_softmax():
    X = np.array([[1, 2], [2, 3], [3, 4]])
    y = np.array([[1, 0], [0, 1], [1, 0]])
    W = np.array([[1, -1], [-1, 1]])
    b = np.array([0, 0])
    expected = cost_softmax(X, y, W, b)  # Self-consistency check
    assert not np.isnan(expected), "Cost test failed"
    print("Cost function test passed.")

# 3.4 Test Gradient Computation
def test_compute_gradient_softmax():
    X = np.array([[1, 2], [3, 4]])
    y = np.array([[1, 0], [0, 1]])
    W = np.array([[0.1, 0.2], [0.3, 0.4]])
    b = np.array([0.01, 0.02])
    grad_W, grad_b = compute_gradient_softmax(X, y, W, b)
    assert grad_W.shape == W.shape, "Gradient W shape mismatch"
    assert grad_b.shape == b.shape, "Gradient b shape mismatch"
    print("Gradient computation test passed.")

# 3.5 Test Prediction Function
def test_predict_softmax():
    np.random.seed(0)
    n, d, c = 10, 5, 3
    X = np.random.rand(n, d)
    W = np.random.rand(d, c)
    b = np.random.rand(c)
    preds = predict_softmax(X, W, b)
    assert preds.shape == (n,), "Prediction shape mismatch"
    assert np.all((preds >= 0) & (preds < c)), "Prediction out of range"
    print("Prediction function test passed.")

# Run all tests
print("Running all helper function tests...")
test_softmax()
test_loss_softmax()
test_cost_softmax()
test_compute_gradient_softmax()
test_predict_softmax()
print("All tests passed successfully.\n")

# ======================
# 4. DATASET PREPARATION (IRIS)
# ======================
iris = load_iris()
X = iris.data
y = iris.target

# One-hot encoding
encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(y.reshape(-1, 1))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y_onehot, test_size=0.2, random_state=42, stratify=y
)

# Standard scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Dataset prepared:")
print(f"Train shape: {X_train_scaled.shape}, Test shape: {X_test_scaled.shape}")

# ======================
# 5. TRAINING SOFTMAX REGRESSION
# ======================
num_classes = y_train.shape[1]
num_features = X_train_scaled.shape[1]
W = np.zeros((num_features, num_classes))
b = np.zeros(num_classes)
alpha = 0.1
n_iter = 1000

print("\nTraining Softmax Regression...")
W, b, cost_history = gradient_descent_softmax(
    X_train_scaled, y_train, W, b, alpha, n_iter, show_cost=True
)

# Plot cost vs iterations
plt.figure(figsize=(9, 6))
plt.plot(cost_history)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("Cost vs Iteration")
plt.grid()
plt.tight_layout()
plt.show()

# ======================
# 6. EVALUATION
# ======================
y_train_pred = predict_softmax(X_train_scaled, W, b)
y_test_pred = predict_softmax(X_test_scaled, W, b)

# Cost on train/test
train_cost = cost_softmax(X_train_scaled, y_train, W, b)
test_cost = cost_softmax(X_test_scaled, y_test, W, b)
print(f"\nTrain Loss: {train_cost:.4f}")
print(f"Test Loss:  {test_cost:.4f}")

# Accuracy
y_test_true = np.argmax(y_test, axis=1)
test_accuracy = np.mean(y_test_pred == y_test_true) * 100
print(f"\nTest Accuracy: {test_accuracy:.2f}%")

# Detailed metrics
cm, precision, recall, f1 = evaluate_classification(y_test_true, y_test_pred)
print("\nConfusion Matrix:")
print(cm)
print(f"Precision: {precision:.2f}")
print(f"Recall:    {recall:.2f}")
print(f"F1-Score:  {f1:.2f}")

# Visualize confusion matrix
fig, ax = plt.subplots(figsize=(6, 6))
ax.imshow(cm, cmap='Blues')
ax.set_xticks(range(3))
ax.set_yticks(range(3))
ax.set_xticklabels(['Pred 0', 'Pred 1', 'Pred 2'])
ax.set_yticklabels(['True 0', 'True 1', 'True 2'])
for i in range(3):
    for j in range(3):
        ax.text(j, i, cm[i, j], ha='center', va='center',
                color='white' if cm[i, j] > np.max(cm) / 2 else 'black')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()